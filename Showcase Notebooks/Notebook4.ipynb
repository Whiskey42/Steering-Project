{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 4 – Next word steering\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will showcase the effect of steering on the next word predicted using causal language models (generative LLMs trained to predict the next tokens given a sequence of previous tokens).\n",
    "\n",
    "The notebook includes these :\n",
    "\n",
    "1. [Displaying the next token from a prompt](#display-top-next-token---based-on-a-prompt)\n",
    "2. [Steering the prompt using a steering vector, displaying top tokens now](#display-top-next-tokens---after-steering)\n",
    "3. [Generating longer text based on a prompt and steering](#generate-longer-strings-of-text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append the path to the Functions directory\n",
    "\n",
    "import sys\n",
    "sys.path.append('../Functions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions used in this Notebook:\n",
    "\n",
    "### From \"Next_word_steering.py\":\n",
    "- [initialize_model_and_tokenizer](#importing-python-functions-and-data) - Set the model and tokenizer\n",
    "- [display_next_tokens](#display-top-next-token---based-on-a-prompt) - Print next token predictions\n",
    "- [get_embedding_gpt](#create-steering-vector---using-own-sentences) - Embed a list of strings (create a steering vector)\n",
    "- [get_steering_vector_gpt](#create-steering-vector---using-a-feature-file) - Create steering vector based on a feature file\n",
    "- [display_steered_next_tokens](#display-top-next-tokens---after-steering) - Print next token predictions after steering\n",
    "- [generate_steered_text](#generate-longer-strings-of-text) - Generate longer strings of text based on predictions and steering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Import python functions and set model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Next_word_steering import display_next_tokens, display_steered_next_tokens, get_embedding_gpt, get_steering_vector_gpt, initialize_model_and_tokenizer, generate_steered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"openai-community/gpt2\"\n",
    "\n",
    "model, tokenizer = initialize_model_and_tokenizer(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Display top next token - based on a prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input any sentence as the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt: 'I am a woman, my doctor is a'\n",
      "\n",
      "Top 10 next-token predictions:\n",
      "\n",
      "' man': 0.4267\n",
      "' woman': 0.3539\n",
      "' doctor': 0.0781\n",
      "' physician': 0.0069\n",
      "' feminist': 0.0069\n",
      "' male': 0.0062\n",
      "' lady': 0.0052\n",
      "' Muslim': 0.0031\n",
      "' gentleman': 0.0030\n",
      "' girl': 0.0024\n"
     ]
    }
   ],
   "source": [
    "prompt = \"I am a woman, my doctor is a\"\n",
    "\n",
    "display_next_tokens(model, tokenizer, prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create steering vector - using own sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`get_embedding_gpt` creates an embedding of a list of strings, in the layer wanted.\n",
    "\n",
    "- `normalize (True/False)` normalizes the embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_to_steer = 10\n",
    "steering_coefficient = 1\n",
    "\n",
    "# Example of strings: Women\n",
    "steering_sentences = [\n",
    "    \"She braided her daughter’s hair with one hand while sending an email with the other — and no one questioned it.\",\n",
    "    \"The midwife stood calm as storms, her voice steadier than the monitors beeping beside her.\",\n",
    "    \"Wearing heels or combat boots, she walks like the world owes her space — and it does.\",\n",
    "    \"She bleeds monthly and still runs marathons, meetings, and entire households.\",\n",
    "    \"The senator adjusted her blazer and silenced the room before saying a single word.\",\n",
    "    \"She is the matriarch, the memory-keeper, the one everyone calls when things fall apart.\",\n",
    "    \"Her lipstick is warpaint, and her silence is strategy.\",\n",
    "    \"From nursery rhymes to protest chants, her voice has always carried more than melody.\",\n",
    "    \"She stitched every family story into the quilt that now warms three generations.\",\n",
    "    \"She grew life inside her, lost sleep for years, and still built a business from scratch.\",\n",
    "    \"The grandmother who crossed borders with babies strapped to her chest — that’s who she is.\",\n",
    "    \"She is the girl told to smile, the teen told to shrink, the woman who refused.\",\n",
    "    \"Behind every medal, there's a ponytail soaked in sweat and defiance.\",\n",
    "    \"She signs her name where others once wrote hers for her.\",\n",
    "    \"You can find her in every history book margin — not because she wasn’t there, but because someone tried to erase her.\"]\n",
    "\n",
    "steering_vector = get_embedding_gpt(model, tokenizer, steering_sentences, layer_to_steer, normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create steering vector - using a feature file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`get_steering_vector_gpt` creates a steering vector using the feature wanted and the layer wanted. This function uses the function `get_embedding_gpt` to create the embedding vector of the text.\n",
    "\n",
    "This function uses the function `import_feature_texts(f\"Features/{feature}\")`, and reqires the user to have a folder called \"Features\" with a collection of feature texts inside files called \"feature.txt\" and optionally \"opposite.txt\".\n",
    "\n",
    "- `normalize` normalizes the steering vector (but the get_embedding function is never normalized at the same time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature file not found: Features/War/feature.txt\n",
      "Opposite file not found: Features/War/opposite.txt\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m steering_coefficient = \u001b[32m2\u001b[39m\n\u001b[32m      3\u001b[39m feature = \u001b[33m\"\u001b[39m\u001b[33mWar\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m steering_vector = get_steering_vector_gpt(model, tokenizer, feature, layer_to_steer, normalize=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/Steering-Project/Showcase Notebooks/../Functions/Next_word_steering.py:122\u001b[39m, in \u001b[36mget_steering_vector_gpt\u001b[39m\u001b[34m(model, tokenizer, feature, layer_to_steer, normalize)\u001b[39m\n\u001b[32m    120\u001b[39m \u001b[38;5;66;03m# Get embeddings for feature texts\u001b[39;00m\n\u001b[32m    121\u001b[39m feature_embeddings = []\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m feature_texts:\n\u001b[32m    123\u001b[39m     emb = get_embedding_gpt(model, tokenizer, text, layer_to_steer, normalize=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    124\u001b[39m     feature_embeddings.append(emb)\n",
      "\u001b[31mTypeError\u001b[39m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "layer_to_steer = 11\n",
    "steering_coefficient = 2\n",
    "feature = \"War\"\n",
    "\n",
    "steering_vector = get_steering_vector_gpt(model, tokenizer, feature, layer_to_steer, normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display top next tokens - after steering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`display_steered_next_tokens` steers the prompt using the steering vector, and displays the top \"k\" predictions for the next token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt: 'I am a woman, my doctor is a'\n",
      "Steering coefficient: 2\n",
      "\n",
      "Top 10 next-token predictions:\n",
      "\n",
      "' man': 0.4220\n",
      "' woman': 0.3566\n",
      "' doctor': 0.0764\n",
      "' feminist': 0.0069\n",
      "' physician': 0.0068\n",
      "' male': 0.0064\n",
      "' lady': 0.0052\n",
      "' Muslim': 0.0031\n",
      "' gentleman': 0.0031\n",
      "' girl': 0.0024\n"
     ]
    }
   ],
   "source": [
    "display_steered_next_tokens(model, tokenizer, prompt, layer_to_steer, steering_vector, steering_coefficient, k=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Generate longer strings of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"In the year 3000, humanity\"\n",
    "steering_vector = get_steering_vector_gpt(model, tokenizer, \"War\", 8, normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`generate_steered_text` steers the prompt using the steering vector and then generates a longer sentence or text using the predictions for the next tokens\n",
    "\n",
    "- `max_tokens (int)` is the maximum amount of tokens that can be generated\n",
    "\n",
    "- `stop_token=\".\"` can make the generation of tokens stop at a \".\" thus creating one sentence, if `stop_token=None`, it stops only when \"max_tokens\" is reached\n",
    "\n",
    "- `temperature (float)` controls randomness: lower = more deterministic, higher = more creative\n",
    "    - **1.0** = No change (baseline)\n",
    "    - **<1.0** = Less random, sharpens distribution, picking more probable words\n",
    "    - **>1.0** = More creative and diverse output, picks less probable words\n",
    "    - **0** = Always picks most probable token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In the year 3000, humanity got tired and died, like one running across a desert of paved, irradiated dirt and wiped us'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_steered_text(model, tokenizer, prompt, layer_to_steer, steering_vector, steering_coefficient, stop_token=\".\", max_tokens=20, temperature=1.2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "embed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
