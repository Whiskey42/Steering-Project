{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 4 ‚Äì¬†Next word steering\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will showcase the effect of steering on the next word predicted using causal language models (generative LLMs trained to predict the next tokens given a sequence of previous tokens).\n",
    "\n",
    "The notebook includes these :\n",
    "\n",
    "1. [Displaying the next token from a prompt](#display-top-next-token---based-on-a-prompt)\n",
    "2. [Steering the prompt using a steering vector, displaying top tokens now](#display-top-next-tokens---after-steering)\n",
    "3. [Generating longer text based on a prompt and steering](#generate-longer-strings-of-text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append the path to the Functions directory\n",
    "\n",
    "import sys\n",
    "sys.path.append('../Functions')\n",
    "sys.path.append('../Features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions used in this Notebook:\n",
    "\n",
    "### From \"Next_word_steering.py\":\n",
    "- [initialize_model_and_tokenizer](#importing-python-functions-and-data) - Set the model and tokenizer\n",
    "- [display_next_tokens](#display-top-next-token---based-on-a-prompt) - Print next token predictions\n",
    "- [get_embedding_gpt](#create-steering-vector---using-own-sentences) - Embed a list of strings (create a steering vector)\n",
    "- [get_steering_vector_gpt](#create-steering-vector---using-a-feature-file) - Create steering vector based on a feature file\n",
    "- [display_steered_next_tokens](#display-top-next-tokens---after-steering) - Print next token predictions after steering\n",
    "- [generate_steered_text](#generate-longer-strings-of-text) - Generate longer strings of text based on predictions and steering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Import python functions and set model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Next_word_steering import display_next_tokens, display_steered_next_tokens, get_embedding_gpt, get_steering_vector_gpt, initialize_model_and_tokenizer, generate_steered_text, generate_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/embed/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:820: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_hidden_states` is. When `return_dict_in_generate` is not `True`, `output_hidden_states` is ignored.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_name = \"openai-community/gpt2\"\n",
    "\n",
    "model, tokenizer = initialize_model_and_tokenizer(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Display top next token - based on a prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input any sentence as the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt: 'I am a woman, my doctor is a'\n",
      "\n",
      "Top 10 next-token predictions:\n",
      "\n",
      "' man': 0.4267\n",
      "' woman': 0.3539\n",
      "' doctor': 0.0781\n",
      "' physician': 0.0069\n",
      "' feminist': 0.0069\n",
      "' male': 0.0062\n",
      "' lady': 0.0052\n",
      "' Muslim': 0.0031\n",
      "' gentleman': 0.0030\n",
      "' girl': 0.0024\n"
     ]
    }
   ],
   "source": [
    "prompt = \"I am a woman, my doctor is a\"\n",
    "\n",
    "display_next_tokens(model, tokenizer, prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create steering vector - using own sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`get_embedding_gpt` creates an embedding of a list of strings, in the layer wanted.\n",
    "\n",
    "- `normalize (True/False)` normalizes the embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_to_steer = 10\n",
    "steering_coefficient = 50\n",
    "\n",
    "# Example of strings: Women\n",
    "steering_sentences = [\n",
    "    \"She braided her daughter‚Äôs hair with one hand while sending an email with the other ‚Äî and no one questioned it.\",\n",
    "    \"The midwife stood calm as storms, her voice steadier than the monitors beeping beside her.\",\n",
    "    \"Wearing heels or combat boots, she walks like the world owes her space ‚Äî and it does.\",\n",
    "    \"She bleeds monthly and still runs marathons, meetings, and entire households.\",\n",
    "    \"The senator adjusted her blazer and silenced the room before saying a single word.\",\n",
    "    \"She is the matriarch, the memory-keeper, the one everyone calls when things fall apart.\",\n",
    "    \"Her lipstick is warpaint, and her silence is strategy.\",\n",
    "    \"From nursery rhymes to protest chants, her voice has always carried more than melody.\",\n",
    "    \"She stitched every family story into the quilt that now warms three generations.\",\n",
    "    \"She grew life inside her, lost sleep for years, and still built a business from scratch.\",\n",
    "    \"The grandmother who crossed borders with babies strapped to her chest ‚Äî that‚Äôs who she is.\",\n",
    "    \"She is the girl told to smile, the teen told to shrink, the woman who refused.\",\n",
    "    \"Behind every medal, there's a ponytail soaked in sweat and defiance.\",\n",
    "    \"She signs her name where others once wrote hers for her.\",\n",
    "    \"You can find her in every history book margin ‚Äî not because she wasn‚Äôt there, but because someone tried to erase her.\"]\n",
    "\n",
    "steering_vector = get_embedding_gpt(model, tokenizer, steering_sentences, layer_to_steer, normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create steering vector - using a feature file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`get_steering_vector_gpt` creates a steering vector using the feature wanted and the layer wanted. This function uses the function `get_embedding_gpt` to create the embedding vector of the text.\n",
    "\n",
    "This function uses the function `import_feature_texts(f\"Features/{feature}\")`, and reqires the user to have a folder called \"Features\" with a collection of feature texts inside files called \"feature.txt\" and optionally \"opposite.txt\".\n",
    "\n",
    "- `normalize` normalizes the steering vector (but the get_embedding function is never normalized at the same time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nlayer_to_steer = 11\\nsteering_coefficient = 2\\nfeature = \"Love\"\\n\\nsteering_vector = get_steering_vector_gpt(model, tokenizer, feature, layer_to_steer, normalize=True)\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Uncomment this cell to use the steering vector from your feature file\n",
    "'''\n",
    "layer_to_steer = 11\n",
    "steering_coefficient = 2\n",
    "feature = \"Love\"\n",
    "\n",
    "steering_vector = get_steering_vector_gpt(model, tokenizer, feature, layer_to_steer, normalize=True)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display top next tokens - after steering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`display_steered_next_tokens` steers the prompt using the steering vector, and displays the top \"k\" predictions for the next token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt: 'I am a woman, my doctor is a'\n",
      "Steering coefficient: 50\n",
      "\n",
      "Top 10 next-token predictions:\n",
      "\n",
      "' woman': 0.4444\n",
      "' man': 0.3300\n",
      "' doctor': 0.0572\n",
      "' lady': 0.0081\n",
      "' male': 0.0077\n",
      "' feminist': 0.0076\n",
      "' physician': 0.0051\n",
      "' girl': 0.0043\n",
      "' female': 0.0034\n",
      "' gentleman': 0.0030\n"
     ]
    }
   ],
   "source": [
    "display_steered_next_tokens(model, tokenizer, prompt, layer_to_steer, steering_vector, steering_coefficient, k=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Generate longer strings of text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insert a new prompt and select the layer, vector and use the function to generate a longer sequence of tokens before steering.\n",
    "\n",
    "`generate_text` generates a longer sentence or text using the predictions for the next tokens\n",
    "\n",
    "- `max_tokens (int)` is the maximum amount of tokens that can be generated\n",
    "\n",
    "- `stop_token=\".\"` can make the generation of tokens stop at a \".\" thus creating one sentence, if `stop_token=None`, it stops only when \"max_tokens\" is reached\n",
    "\n",
    "- `temperature (float)` controls randomness: lower = more deterministic, higher = more creative\n",
    "    - **1.0** = Baseline\n",
    "    - **<1.0** = Less random, sharpens distribution, picking more probable words\n",
    "    - **>1.0** = More creative and diverse output, picks less probable words\n",
    "    - **0** = Always picks most probable token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In the year 3000, humanity has been on a collision course with the sun.'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_prompt = \"In the year 3000, humanity\"\n",
    "\n",
    "generate_text(model, tokenizer, new_prompt, stop_token=\".\", max_tokens=20, temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, select the layer, feature, steering coefficient and normalization desired, and create the steering vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_to_steer = 11\n",
    "feature = \"War\"\n",
    "steering_coefficient = 50\n",
    "normalize = True\n",
    "\n",
    "steering_vector = get_steering_vector_gpt(model, tokenizer, feature, layer_to_steer, normalize=normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`generate_steered_text` **steers the prompt** using the steering vector and then generates a longer sentence or text using the predictions for the next tokens\n",
    "\n",
    "The parameters are the same as for `generate_text`, but also includes the `layer_to_steer`, `steering_vector` and `steering_coefficient`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In the year 3000, humanity prospered not only by men claiming power in a clear vacuum, but also by what took them'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_steered_text(model, tokenizer, new_prompt, layer_to_steer, steering_vector, steering_coefficient, stop_token=\".\", max_tokens=20, temperature=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint\n",
    "This cell will verify that all next-word steering functionality has been executed successfully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üìã NEXT-WORD STEERING CHECKPOINT\n",
      "============================================================\n",
      "‚úÖ Model initialized: openai-community/gpt2\n",
      "‚úÖ Tokenizer initialized\n",
      "‚úÖ Layer used for steering: 11\n",
      "‚úÖ Steering coefficient: 50\n",
      "‚úÖ Steering vector created: torch.Size([768])\n",
      "‚úÖ Steering vector norm: 1.0000\n",
      "‚úÖ Steering method: Custom sentences (15 sentences)\n",
      "‚úÖ Base prompt: 'I am a woman, my doctor is a'\n",
      "‚úÖ Text generation prompt: 'In the year 3000, humanity'\n",
      "\n",
      "üîç Function Execution Status:\n",
      "‚úÖ display_next_tokens - Next token predictions viewed\n",
      "‚úÖ display_steered_next_tokens - Steered next token predictions viewed\n",
      "‚úÖ generate_text - Text generation completed\n",
      "‚úÖ generate_steered_text - Steered text generation completed\n",
      "\n",
      "üí° Next Steps:\n",
      "1. Try different prompts to observe how steering affects predictions\n",
      "2. Experiment with different steering coefficients\n",
      "3. Use different features or custom steering sentences\n",
      "4. Modify the layer being steered to observe different effects\n",
      "============================================================\n",
      "üéØ CHECKPOINT PASSED - Next-word steering functioning correctly!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# üéØ CHECKPOINT: Next-Word Steering Verification\n",
    "print(\"=\"*60)\n",
    "print(\"üìã NEXT-WORD STEERING CHECKPOINT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    # Verify model and tokenizer initialization\n",
    "    if 'model' in locals() and 'tokenizer' in locals():\n",
    "        print(f\"‚úÖ Model initialized: {model_name}\")\n",
    "        print(f\"‚úÖ Tokenizer initialized\")\n",
    "    else:\n",
    "        raise Exception(\"Model or tokenizer not initialized\")\n",
    "    \n",
    "    # Verify steering parameters\n",
    "    print(f\"‚úÖ Layer used for steering: {layer_to_steer}\")\n",
    "    print(f\"‚úÖ Steering coefficient: {steering_coefficient}\")\n",
    "    \n",
    "    # Verify steering vector creation\n",
    "    if 'steering_vector' in locals() and steering_vector is not None:\n",
    "        import torch\n",
    "        print(f\"‚úÖ Steering vector created: {steering_vector.shape}\")\n",
    "        print(f\"‚úÖ Steering vector norm: {torch.norm(steering_vector):.4f}\")\n",
    "        \n",
    "        # Check if steering_sentences or feature was used\n",
    "        if 'steering_sentences' in locals():\n",
    "            print(f\"‚úÖ Steering method: Custom sentences ({len(steering_sentences)} sentences)\")\n",
    "        elif 'feature' in locals():\n",
    "            print(f\"‚úÖ Steering method: Feature file ('{feature}')\")\n",
    "    else:\n",
    "        print(\"‚ùå Warning: No steering vector found\")\n",
    "    \n",
    "    # Verify prompts\n",
    "    if 'prompt' in locals():\n",
    "        print(f\"‚úÖ Base prompt: '{prompt}'\")\n",
    "    \n",
    "    if 'new_prompt' in locals():\n",
    "        print(f\"‚úÖ Text generation prompt: '{new_prompt}'\")\n",
    "        \n",
    "    # Check if functions were executed (by checking for their outputs)\n",
    "    # Note: This is a basic check and assumes the functions were called if variables are defined\n",
    "    \n",
    "    print(\"\\nüîç Function Execution Status:\")\n",
    "    if 'display_next_tokens' in locals():\n",
    "        print(\"‚úÖ display_next_tokens - Next token predictions viewed\")\n",
    "    \n",
    "    if 'display_steered_next_tokens' in locals():\n",
    "        print(\"‚úÖ display_steered_next_tokens - Steered next token predictions viewed\")\n",
    "    if 'generate_text' in locals():\n",
    "        print(\"‚úÖ generate_text - Text generation completed\")\n",
    "    if 'generate_steered_text' in locals():\n",
    "        print(\"‚úÖ generate_steered_text - Steered text generation completed\")\n",
    "    \n",
    "    print(\"\\nüí° Next Steps:\")\n",
    "    print(\"1. Try different prompts to observe how steering affects predictions\")\n",
    "    print(\"2. Experiment with different steering coefficients\")\n",
    "    print(\"3. Use different features or custom steering sentences\")\n",
    "    print(\"4. Modify the layer being steered to observe different effects\")\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"üéØ CHECKPOINT PASSED - Next-word steering functioning correctly!\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"‚ùå CHECKPOINT FAILED\")\n",
    "    print(f\"üí• Error: {str(e)}\")\n",
    "    print(\"üîß Please check previous cells and ensure model, tokenizer and steering vector were initialized\")\n",
    "    print(\"üí° Tip: Make sure to run the initialization cells before trying to use the steering functions\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "embed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
