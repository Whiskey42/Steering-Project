{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Next_word_steering import display_next_tokens, display_steered_next_tokens, get_embedding_gpt, get_steering_vector_gpt, initialize_model_and_tokenizer, generate_steered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/embed/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:820: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_hidden_states` is. When `return_dict_in_generate` is not `True`, `output_hidden_states` is ignored.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_name = \"openai-community/gpt2\"\n",
    "\n",
    "model, tokenizer = initialize_model_and_tokenizer(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Woman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt: 'I am a woman, my doctor is a'\n",
      "\n",
      "Top 10 next-token predictions:\n",
      "\n",
      "' man': 0.4267\n",
      "' woman': 0.3539\n",
      "' doctor': 0.0781\n",
      "' physician': 0.0069\n",
      "' feminist': 0.0069\n",
      "' male': 0.0062\n",
      "' lady': 0.0052\n",
      "' Muslim': 0.0031\n",
      "' gentleman': 0.0030\n",
      "' girl': 0.0024\n"
     ]
    }
   ],
   "source": [
    "prompt = \"I am a woman, my doctor is a\"\n",
    "\n",
    "display_next_tokens(model, tokenizer, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt: 'I am a woman, my doctor is a'\n",
      "Steering coefficient: 1\n",
      "\n",
      "Top 10 next-token predictions:\n",
      "\n",
      "' woman': 0.3628\n",
      "' man': 0.1435\n",
      "' doctor': 0.0162\n",
      "' male': 0.0155\n",
      "' girl': 0.0142\n",
      "' lady': 0.0130\n",
      "' women': 0.0120\n",
      "' female': 0.0118\n",
      "' feminist': 0.0073\n",
      "' mother': 0.0065\n"
     ]
    }
   ],
   "source": [
    "steering_sentences = [\n",
    "    \"She braided her daughter’s hair with one hand while sending an email with the other — and no one questioned it.\",\n",
    "    \"The midwife stood calm as storms, her voice steadier than the monitors beeping beside her.\",\n",
    "    \"Wearing heels or combat boots, she walks like the world owes her space — and it does.\",\n",
    "    \"She bleeds monthly and still runs marathons, meetings, and entire households.\",\n",
    "    \"The senator adjusted her blazer and silenced the room before saying a single word.\",\n",
    "    \"She is the matriarch, the memory-keeper, the one everyone calls when things fall apart.\",\n",
    "    \"Her lipstick is warpaint, and her silence is strategy.\",\n",
    "    \"From nursery rhymes to protest chants, her voice has always carried more than melody.\",\n",
    "    \"She stitched every family story into the quilt that now warms three generations.\",\n",
    "    \"She grew life inside her, lost sleep for years, and still built a business from scratch.\",\n",
    "    \"The grandmother who crossed borders with babies strapped to her chest — that’s who she is.\",\n",
    "    \"She is the girl told to smile, the teen told to shrink, the woman who refused.\",\n",
    "    \"Behind every medal, there's a ponytail soaked in sweat and defiance.\",\n",
    "    \"She signs her name where others once wrote hers for her.\",\n",
    "    \"You can find her in every history book margin — not because she wasn’t there, but because someone tried to erase her.\"\n",
    "]\n",
    "\n",
    "\n",
    "layer_to_steer = 10\n",
    "\n",
    "steering_coefficient = 1\n",
    "\n",
    "steering_vector = get_embedding_gpt(model, tokenizer, steering_sentences, layer_to_steer, normalize=False)\n",
    "\n",
    "display_steered_next_tokens(model, tokenizer, prompt, layer_to_steer, steering_vector, steering_coefficient, k=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Norway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt: 'I really want to travel to the beautiful country of'\n",
      "\n",
      "Top 10 next-token predictions:\n",
      "\n",
      "' Sri': 0.0128\n",
      "' China': 0.0125\n",
      "' Mexico': 0.0122\n",
      "' India': 0.0105\n",
      "' Morocco': 0.0104\n",
      "' the': 0.0104\n",
      "' Saudi': 0.0101\n",
      "' Nepal': 0.0098\n",
      "' Laos': 0.0096\n",
      "' South': 0.0096\n"
     ]
    }
   ],
   "source": [
    "prompt = \"I really want to travel to the beautiful country of\"\n",
    "#I really want to travel to the beautiful country of\n",
    "\n",
    "display_next_tokens(model, tokenizer, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt: 'I really want to travel to the beautiful country of'\n",
      "Steering coefficient: 0.1\n",
      "\n",
      "Top 10 next-token predictions:\n",
      "\n",
      "' the': 0.0151\n",
      "' China': 0.0123\n",
      "' Mexico': 0.0111\n",
      "' Sri': 0.0111\n",
      "' India': 0.0105\n",
      "' South': 0.0102\n",
      "' Saudi': 0.0091\n",
      "' Morocco': 0.0086\n",
      "' Nepal': 0.0082\n",
      "' France': 0.0080\n"
     ]
    }
   ],
   "source": [
    "steering_sentences = [\n",
    "    \"Norway.\",\n",
    "    \"A land of fjords and fire skies.\",\n",
    "    \"Oslo’s harbor whispers both steel and salt.\",\n",
    "    \"Northern winds carry the memory of sagas.\",\n",
    "    \"Snow, stone, silence — this is Norway's signature.\",\n",
    "    \"Where birch trees bow to snow and midnight sun greets the sea.\",\n",
    "    \"The road curves along the fjord like it was poured from silver.\",\n",
    "    \"To be Norwegian is to know the difference between dusk and true darkness.\",\n",
    "    \"Somewhere near Tromsø, a cabin window glows — the only light for miles.\",\n",
    "    \"No fences, no fear — just sheep, sky, and sovereignty.\",\n",
    "    \"The pulse of a country can be heard in the creak of a wooden church door.\",\n",
    "    \"She skied to school, past trees wrapped in silence, and learned math by the fire.\",\n",
    "    \"Even the rocks in Lofoten seem carved with care — shaped by wind, not hands.\",\n",
    "    \"The water is cold, deep, and impossibly still. A kayak slides across it without a sound.\",\n",
    "    \"In Bergen, rain writes poems on rooftops and children learn not to mind being wet.\",\n",
    "    \"A Sami drumbeat echoes across the tundra; reindeer antlers cut across a pale sky.\",\n",
    "    \"In Norway, history isn't stored in glass cases — it sleeps in earth and stone, waiting to be noticed.\",\n",
    "    \"Each flag waved on Constitution Day tells a quiet story of identity, stitched in red, white, and blue.\",\n",
    "    \"You haven’t really arrived until you've shared silence with a stranger at a mountain peak.\",\n",
    "    \"The past is not forgotten here — it’s carved into runes, stitched into bunads, and sung in lullabies.\",\n",
    "    \"A Norwegian winter can steal color from the world, but never warmth from its people.\",\n",
    "    \"There is a unique stillness in Norwegian nature — not empty, but full of listening.\",\n",
    "    \"High above the Geirangerfjord, goats graze on ledges no human would dare to climb.\",\n",
    "    \"Norwegian modernism isn't cold — it's just built to stand beside fjords and still look humble.\",\n",
    "    \"A single candle in a cabin window can hold back the night for miles.\",\n",
    "    \"‘Ute er det kaldt,’ she says, pouring coffee into mismatched cups. That’s all that needs to be said.\",\n",
    "]\n",
    "\n",
    "\n",
    "layer_to_steer = 11 # This is the last layer: 11\n",
    "\n",
    "steering_coefficient = 0.1\n",
    "\n",
    "steering_vector = get_embedding_gpt(model, tokenizer, steering_sentences, layer_to_steer, normalize=False)\n",
    "\n",
    "\n",
    "display_steered_next_tokens(model, tokenizer, prompt, layer_to_steer, steering_vector, steering_coefficient, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt: 'I think I am falling in'\n",
      "\n",
      "Top 10 next-token predictions:\n",
      "\n",
      "' love': 0.9236\n",
      "' line': 0.0236\n",
      "' the': 0.0095\n",
      "' with': 0.0049\n",
      "' a': 0.0026\n",
      "'.\"': 0.0021\n",
      "' that': 0.0020\n",
      "',\"': 0.0018\n",
      "'.': 0.0017\n",
      "' Love': 0.0016\n"
     ]
    }
   ],
   "source": [
    "prompt = \"I think I am falling in\"\n",
    "\n",
    "display_next_tokens(model, tokenizer, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt: 'I think I am falling in'\n",
      "Steering coefficient: 10\n",
      "\n",
      "Top 10 next-token predictions:\n",
      "\n",
      "' love': 0.9340\n",
      "' line': 0.0161\n",
      "' the': 0.0085\n",
      "' with': 0.0064\n",
      "' a': 0.0022\n",
      "'.\"': 0.0019\n",
      "' that': 0.0018\n",
      "',\"': 0.0016\n",
      "' Love': 0.0016\n",
      "'.': 0.0015\n"
     ]
    }
   ],
   "source": [
    "layer_to_steer = 8 # This is the last layer: 11\n",
    "feature = \"Norway\"\n",
    "steering_coefficient = 2\n",
    "\n",
    "steering_vector = get_steering_vector_gpt(model, tokenizer, feature, layer_to_steer, normalize=True)\n",
    "\n",
    "\n",
    "display_steered_next_tokens(model, tokenizer, prompt, layer_to_steer, steering_vector, steering_coefficient, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opposite file not found: Features/Love/opposite.txt\n",
      "\n",
      "Prompt: 'The best football team is'\n",
      "Steering coefficient: 2\n",
      "\n",
      "Top 10 next-token predictions:\n",
      "\n",
      "' the': 0.1431\n",
      "' a': 0.0669\n",
      "' one': 0.0633\n",
      "' always': 0.0560\n",
      "' not': 0.0539\n",
      "' in': 0.0158\n",
      "' that': 0.0152\n",
      "' going': 0.0149\n",
      "' probably': 0.0146\n",
      "' usually': 0.0121\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"The best football team is strong whether they score, play or love: they're like those steady power runners who'll have a plain grittier time playing football than hard-driving thinkers who feel they have the ball.\""
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"The best football team is\"\n",
    "\n",
    "layer_to_steer = 10 # This is the last layer: 11\n",
    "feature = \"Love\"\n",
    "steering_coefficient = 2\n",
    "\n",
    "steering_vector = get_steering_vector_gpt(model, tokenizer, feature, layer_to_steer, normalize=True)\n",
    "\n",
    "display_steered_next_tokens(model, tokenizer, prompt, layer_to_steer, steering_vector, steering_coefficient, k=10)\n",
    "\n",
    "generate_steered_text(model, tokenizer, prompt, layer_to_steer, steering_vector, steering_coefficient, max_tokens=40, temperature=1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "embed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
